import torch
import torch.nn as nn
import torch.optim as optim

# --- 1. The Atomic Unit: A Single Memory Head ---
class MemoryHead(nn.Module):
    def __init__(self, hidden_dim, decay_rate):
        super().__init__()
        self.gamma = decay_rate
        self.hidden_dim = hidden_dim
        
        # Each head has its OWN "Neural Optimizer" 
        # (A specialized Guide for this specific timescale)
        self.optimizer_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim * hidden_dim) # Output full matrix
        )

    def forward(self, x_input, memory_matrix):
        # A. Predict
        pred = torch.matmul(memory_matrix, x_input)
        
        # B. Error
        error = (pred - x_input).detach()
        
        # C. Propose Update (The "Lesson")
        context = torch.cat([error, x_input], dim=-1)
        raw_update = self.optimizer_mlp(context)
        update_matrix = raw_update.view(self.hidden_dim, self.hidden_dim)
        update_proposal = torch.tanh(update_matrix)
        
        # D. Update Memory (The "Commit")
        # Apply this head's specific decay rate (gamma)
        new_memory = (self.gamma * memory_matrix) + \
                     ((1 - self.gamma) * update_proposal)
        
        return pred, new_memory

# --- 2. The Complex Architecture: Multi-Scale Memory ---
class MultiScaleHOPE(nn.Module):
    def __init__(self, hidden_dim, gammas=[0.0, 0.5, 0.9, 0.99]):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.gammas = gammas
        
        # Create a list of heads (The Team)
        # We use ModuleList so PyTorch knows to train them all
        self.heads = nn.ModuleList([
            MemoryHead(hidden_dim, g) for g in gammas
        ])
        
        # The Final Mixer
        # Combines the opinions of all 4 heads into one final answer
        self.output_projection = nn.Linear(hidden_dim * len(gammas), hidden_dim)

    def forward(self, x_input, memory_states):
        """
        x_input: [batch, hidden_dim]
        memory_states: A list of matrices, one for each head
        """
        head_outputs = []
        new_memory_states = []
        
        # Run each head in parallel
        for i, head in enumerate(self.heads):
            # Get the memory belonging to THIS head
            prev_mem = memory_states[i]
            
            # Run the head
            pred, new_mem = head(x_input, prev_mem)
            
            # Store results
            head_outputs.append(pred)
            new_memory_states.append(new_mem)
            
        # Concatenate all predictions: [Head1_out, Head2_out, ...]
        combined_view = torch.cat(head_outputs, dim=-1)
        
        # Mix them down to the final size
        final_prediction = self.output_projection(combined_view)
        
        return final_prediction, new_memory_states

    def init_memory(self):
        # Helper to create fresh empty memories for all heads
        return [torch.zeros(self.hidden_dim, self.hidden_dim) for _ in self.gammas]

# --- 3. The Training Loop ---
torch.manual_seed(42)
hidden_dim = 16

# Define our Team of Experts (4 distinct timescales)
model = MultiScaleHOPE(hidden_dim=hidden_dim, gammas=[0.0, 0.5, 0.9, 0.995])

meta_optimizer = optim.Adam(model.parameters(), lr=0.01)

# Initialize List of Memories (One for each head)
memory_states = model.init_memory()

# Setup Data (Repeating Pattern)
vocab_size = 5
dictionary = [torch.randn(hidden_dim) for _ in range(vocab_size)]

print(f"{'Step':<10} | {'Loss':<10} | {'Action'}")
print("-" * 35)

for step in range(300):
    # Data
    word_index = step % vocab_size 
    input_vector = dictionary[word_index]

    # Forward
    prediction, memory_states = model(input_vector, memory_states)

    # Loss
    loss = torch.mean((prediction - input_vector)**2)

    # Backward
    meta_optimizer.zero_grad()
    loss.backward()
    meta_optimizer.step()

    # Detach ALL memory states
    memory_states = [m.detach() for m in memory_states]

    if step % 20 == 0:
         print(f"{step:<10} | {loss.item():.4f}     | Learning...")